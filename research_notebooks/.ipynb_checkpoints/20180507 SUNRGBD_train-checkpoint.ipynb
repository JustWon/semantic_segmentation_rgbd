{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import collections\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import scipy.misc as m\n",
    "import scipy.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "def recursive_glob(rootdir='.', suffix=''):\n",
    "    \"\"\"Performs recursive glob with given suffix and rootdir \n",
    "        :param rootdir is the root directory\n",
    "        :param suffix is the suffix to be searched\n",
    "    \"\"\"\n",
    "    return [os.path.join(looproot, filename)\n",
    "        for looproot, _, filenames in os.walk(rootdir)\n",
    "        for filename in filenames if filename.endswith(suffix)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SUNRGBDLoader(data.Dataset):\n",
    "    def __init__(self, root, split=\"training\", is_transform=False, img_size=(480, 640), img_norm=True):\n",
    "        self.root = root\n",
    "        self.is_transform = is_transform\n",
    "        self.n_classes = 38\n",
    "        self.img_norm = img_norm\n",
    "        self.img_size = img_size if isinstance(img_size, tuple) else (img_size, img_size)\n",
    "        self.mean = np.array([104.00699, 116.66877, 122.67892])\n",
    "        self.color_files = collections.defaultdict(list)\n",
    "        self.depth_files = collections.defaultdict(list)\n",
    "        self.semantic_files = collections.defaultdict(list)\n",
    "        self.cmap = self.color_map(normalized=False)\n",
    "\n",
    "        split_map = {\"training\": 'train', \"val\": 'test',}\n",
    "        self.split = split_map[split]\n",
    "\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            file_list =  sorted(recursive_glob(rootdir=self.root + '/color/', suffix='jpg'))\n",
    "            self.color_files[split] = file_list\n",
    "        \n",
    "        for split in [\"train\", \"test\"]:\n",
    "            file_list =  sorted(recursive_glob(rootdir=self.root + '/depth/', suffix='png'))\n",
    "            self.depth_files[split] = file_list    \n",
    "        \n",
    "        for split in [\"train\", \"test\"]:\n",
    "            file_list =  sorted(recursive_glob(rootdir=self.root + '/seg/', suffix='png'))\n",
    "            self.semantic_files[split] = file_list\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.color_files[self.split])\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        color_path = self.color_files[self.split][index].rstrip()\n",
    "        depth_path = self.depth_files[self.split][index].rstrip()\n",
    "        semantic_path = self.semantic_files[self.split][index].rstrip()\n",
    "\n",
    "        color_img = m.imread(color_path)    \n",
    "        color_img = np.array(color_img, dtype=np.uint8)\n",
    "\n",
    "        depth_img = m.imread(depth_path)    \n",
    "        depth_img = np.array(depth_img, dtype=np.uint8)\n",
    "        \n",
    "        semantic_img = m.imread(semantic_path)\n",
    "        semantic_img = np.array(semantic_img, dtype=np.uint8)\n",
    "        \n",
    "        if self.is_transform:\n",
    "            color_img, semantic_img = self.transform(color_img, semantic_img)\n",
    "        \n",
    "        return color_img, depth_img, semantic_img\n",
    "\n",
    "\n",
    "    def transform(self, img, lbl):\n",
    "        img = m.imresize(img, (self.img_size[0], self.img_size[1])) # uint8 with RGB mode\n",
    "        img = img[:, :, ::-1] # RGB -> BGR\n",
    "        img = img.astype(np.float64)\n",
    "        img -= self.mean\n",
    "        if self.img_norm:\n",
    "            # Resize scales images from 0 to 255, thus we need\n",
    "            # to divide by 255.0\n",
    "            img = img.astype(float) / 255.0\n",
    "        # NHWC -> NCHW\n",
    "        img = img.transpose(2, 0, 1)\n",
    "\n",
    "        classes = np.unique(lbl)\n",
    "        lbl = lbl.astype(float)\n",
    "        lbl = m.imresize(lbl, (self.img_size[0], self.img_size[1]), 'nearest', mode='F')\n",
    "        lbl = lbl.astype(int)\n",
    "        assert(np.all(classes == np.unique(lbl)))\n",
    "\n",
    "        img = torch.from_numpy(img).float()\n",
    "        lbl = torch.from_numpy(lbl).long()\n",
    "        return img, lbl\n",
    "\n",
    "\n",
    "    def color_map(self, N=256, normalized=False):\n",
    "        \"\"\"\n",
    "        Return Color Map in PASCAL VOC format\n",
    "        \"\"\"\n",
    "\n",
    "        def bitget(byteval, idx):\n",
    "            return ((byteval & (1 << idx)) != 0)\n",
    "\n",
    "        dtype = 'float32' if normalized else 'uint8'\n",
    "        cmap = np.zeros((N, 3), dtype=dtype)\n",
    "        for i in range(N):\n",
    "            r = g = b = 0\n",
    "            c = i\n",
    "            for j in range(8):\n",
    "                r = r | (bitget(c, 0) << 7-j)\n",
    "                g = g | (bitget(c, 1) << 7-j)\n",
    "                b = b | (bitget(c, 2) << 7-j)\n",
    "                c = c >> 3\n",
    "\n",
    "            cmap[i] = np.array([r, g, b])\n",
    "\n",
    "        cmap = cmap/255.0 if normalized else cmap\n",
    "        return cmap\n",
    "\n",
    "\n",
    "    def decode_segmap(self, temp):\n",
    "        r = temp.copy()\n",
    "        g = temp.copy()\n",
    "        b = temp.copy()\n",
    "        for l in range(0, self.n_classes):\n",
    "            r[temp == l] = self.cmap[l,0]\n",
    "            g[temp == l] = self.cmap[l,1]\n",
    "            b[temp == l] = self.cmap[l,2]\n",
    "\n",
    "        rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n",
    "        rgb[:, :, 0] = r / 255.0\n",
    "        rgb[:, :, 1] = g / 255.0\n",
    "        rgb[:, :, 2] = b / 255.0\n",
    "        return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "local_path = '/home/dongwonshin/Desktop/Datasets/SUN_RGBD(organized)/xtion'\n",
    "dst = SUNRGBDLoader(local_path, is_transform=True)\n",
    "bs = 2\n",
    "trainloader = data.DataLoader(dst, batch_size=bs, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid dimensions for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-49008f422ac9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0maxarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0maxarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0maxarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msemantic_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1890\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1891\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1892\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1893\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1894\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5116\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5118\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5119\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    547\u001b[0m         if (self._A.ndim not in (2, 3) or\n\u001b[1;32m    548\u001b[0m                 (self._A.ndim == 3 and self._A.shape[-1] not in (3, 4))):\n\u001b[0;32m--> 549\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid dimensions for image data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_imcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid dimensions for image data"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8d83fada58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader):\n",
    "    color_img, depth_img, semantic_img = data\n",
    "    color_img = color_img.numpy()\n",
    "    depth_img = depth_img.numpy()\n",
    "    semantic_img = semantic_img.numpy()\n",
    "    f, axarr = plt.subplots(bs,3)\n",
    "    \n",
    "    for j in range(bs):      \n",
    "        axarr[j][0].imshow(color_img[j])\n",
    "        axarr[j][1].imshow(depth_img[j])\n",
    "        axarr[j][2].imshow(semantic_img[j])\n",
    "    plt.show()\n",
    "#     break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import torch\n",
    "import visdom\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ptsemseg.models import get_model\n",
    "from ptsemseg.loader import get_loader, get_data_path\n",
    "from ptsemseg.metrics import runningScore\n",
    "from ptsemseg.loss import *\n",
    "from ptsemseg.augmentations import *\n",
    "\n",
    "def train(args):\n",
    "\n",
    "    # Setup Augmentations\n",
    "    data_aug= Compose([RandomRotate(10),                                        \n",
    "                       RandomHorizontallyFlip()])\n",
    "\n",
    "    # Setup Dataloader\n",
    "    data_path = '/home/dongwonshin/Desktop/Datasets/SUN_RGBD(organized)/xtion'\n",
    "    t_loader = SUNRGBDLoader(data_path, is_transform=True)\n",
    "    v_loader = SUNRGBDLoader(data_path, is_transform=True, split='val')\n",
    "\n",
    "    n_classes = t_loader.n_classes\n",
    "    trainloader = data.DataLoader(t_loader, batch_size=args.batch_size, num_workers=8, shuffle=True)\n",
    "    valloader = data.DataLoader(v_loader, batch_size=args.batch_size, num_workers=8)\n",
    "\n",
    "    # Setup Metrics\n",
    "    running_metrics = runningScore(n_classes)\n",
    "        \n",
    "    # Setup visdom for visualization\n",
    "    if args.visdom:\n",
    "        vis = visdom.Visdom()\n",
    "\n",
    "        loss_window = vis.line(X=torch.zeros((1,)).cpu(),\n",
    "                           Y=torch.zeros((1)).cpu(),\n",
    "                           opts=dict(xlabel='minibatches',\n",
    "                                     ylabel='Loss',\n",
    "                                     title='Training Loss',\n",
    "                                     legend=['Loss']))\n",
    "\n",
    "    # Setup Model\n",
    "    model = get_model(args.arch, n_classes)\n",
    "    \n",
    "    model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "    model.cuda()\n",
    "    \n",
    "    # Check if model has custom optimizer / loss\n",
    "    if hasattr(model.module, 'optimizer'):\n",
    "        optimizer = model.module.optimizer\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=args.l_rate, momentum=0.99, weight_decay=5e-4)\n",
    "\n",
    "    if hasattr(model.module, 'loss'):\n",
    "        print('Using custom loss')\n",
    "        loss_fn = model.module.loss\n",
    "    else:\n",
    "        loss_fn = cross_entropy2d\n",
    "\n",
    "    if args.resume is not None:                                         \n",
    "        if os.path.isfile(args.resume):\n",
    "            print(\"Loading model and optimizer from checkpoint '{}'\".format(args.resume))\n",
    "            checkpoint = torch.load(args.resume)\n",
    "            model.load_state_dict(checkpoint['model_state'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "            print(\"Loaded checkpoint '{}' (epoch {})\"                    \n",
    "                  .format(args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"No checkpoint found at '{}'\".format(args.resume)) \n",
    "\n",
    "    best_iou = -100.0 \n",
    "    for epoch in range(args.n_epoch):\n",
    "        model.train()\n",
    "        for i, (color_imgs, depth_imgs, semantic_imgs) in enumerate(trainloader):\n",
    "            images = Variable(color_imgs.cuda())\n",
    "            labels = Variable(semantic_imgs.cuda())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = loss_fn(input=outputs, target=labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if args.visdom:\n",
    "                vis.line(\n",
    "                    X=torch.ones((1, 1)).cpu() * i,\n",
    "                    Y=torch.Tensor([loss.data[0]]).unsqueeze(0).cpu(),\n",
    "                    win=loss_window,\n",
    "                    update='append')\n",
    "\n",
    "            if (i+1) % 20 == 0:\n",
    "                print(\"Epoch [%d/%d] Loss: %.4f\" % (epoch+1, args.n_epoch, loss.data[0]))\n",
    "\n",
    "        model.eval()\n",
    "        for i_val, (images_val, labels_val) in tqdm(enumerate(valloader)):\n",
    "            images_val = Variable(images_val.cuda(), volatile=True)\n",
    "            labels_val = Variable(labels_val.cuda(), volatile=True)\n",
    "\n",
    "            outputs = model(images_val)\n",
    "            pred = outputs.data.max(1)[1].cpu().numpy()\n",
    "            gt = labels_val.data.cpu().numpy()\n",
    "            running_metrics.update(gt, pred)\n",
    "\n",
    "        score, class_iou = running_metrics.get_scores()\n",
    "        for k, v in score.items():\n",
    "            print(k, v)\n",
    "        running_metrics.reset()\n",
    "\n",
    "        if score['Mean IoU : \\t'] >= best_iou:\n",
    "            best_iou = score['Mean IoU : \\t']\n",
    "            state = {'epoch': epoch+1,\n",
    "                     'model_state': model.state_dict(),\n",
    "                     'optimizer_state' : optimizer.state_dict(),}\n",
    "            torch.save(state, \"{}_{}_best_model.pkl\".format(args.arch, args.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import torch\n",
    "import visdom\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ptsemseg.models import get_model\n",
    "from ptsemseg.loader import get_loader, get_data_path\n",
    "from ptsemseg.metrics import runningScore\n",
    "from ptsemseg.loss import *\n",
    "from ptsemseg.augmentations import *\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Hyperparams')\n",
    "parser.add_argument('--arch', nargs='?', type=str, default='fcn8s', \n",
    "                    help='Architecture to use [\\'fcn8s, unet, segnet etc\\']')\n",
    "parser.add_argument('--img_rows', nargs='?', type=int, default=256, \n",
    "                    help='Height of the input image')\n",
    "parser.add_argument('--img_cols', nargs='?', type=int, default=256, \n",
    "                    help='Width of the input image')\n",
    "\n",
    "parser.add_argument('--img_norm', dest='img_norm', action='store_true', \n",
    "                    help='Enable input image scales normalization [0, 1] | True by default')\n",
    "parser.add_argument('--no-img_norm', dest='img_norm', action='store_false', \n",
    "                    help='Disable input image scales normalization [0, 1] | True by default')\n",
    "parser.set_defaults(img_norm=True)\n",
    "\n",
    "parser.add_argument('--n_epoch', nargs='?', type=int, default=100, \n",
    "                    help='# of the epochs')\n",
    "parser.add_argument('--batch_size', nargs='?', type=int, default=1, \n",
    "                    help='Batch Size')\n",
    "parser.add_argument('--l_rate', nargs='?', type=float, default=1e-5, \n",
    "                    help='Learning Rate')\n",
    "parser.add_argument('--feature_scale', nargs='?', type=int, default=1, \n",
    "                    help='Divider for # of features to use')\n",
    "parser.add_argument('--resume', nargs='?', type=str, default=None,    \n",
    "                    help='Path to previous saved model to restart from')\n",
    "\n",
    "parser.add_argument('--visdom', dest='visdom', action='store_true', \n",
    "                    help='Enable visualization(s) on visdom | False by default')\n",
    "parser.add_argument('--no-visdom', dest='visdom', action='store_false', \n",
    "                    help='Disable visualization(s) on visdom | False by default')\n",
    "parser.set_defaults(visdom=False)\n",
    "\n",
    "# args = parser.parse_args('--arch fcn8s'.split(' '))\n",
    "# train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'fcn8s' object has no attribute 'module'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b8b63e9d3c13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Check if model has custom optimizer / loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 398\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'fcn8s' object has no attribute 'module'"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args('--arch fcn8s'.split(' '))\n",
    "\n",
    "# Setup Augmentations\n",
    "data_aug= Compose([RandomRotate(10),                                        \n",
    "                   RandomHorizontallyFlip()])\n",
    "\n",
    "# Setup Dataloader\n",
    "data_path = '/home/dongwonshin/Desktop/Datasets/SUN_RGBD(organized)/xtion'\n",
    "t_loader = SUNRGBDLoader(data_path, is_transform=True)\n",
    "v_loader = SUNRGBDLoader(data_path, is_transform=True, split='val')\n",
    "\n",
    "n_classes = t_loader.n_classes\n",
    "trainloader = data.DataLoader(t_loader, batch_size=args.batch_size, num_workers=16, shuffle=True)\n",
    "valloader = data.DataLoader(v_loader, batch_size=args.batch_size, num_workers=16)\n",
    "\n",
    "# Setup Metrics\n",
    "running_metrics = runningScore(n_classes)\n",
    "\n",
    "# Setup visdom for visualization\n",
    "if args.visdom:\n",
    "    vis = visdom.Visdom()\n",
    "\n",
    "    loss_window = vis.line(X=torch.zeros((1,)).cpu(),\n",
    "                       Y=torch.zeros((1)).cpu(),\n",
    "                       opts=dict(xlabel='minibatches',\n",
    "                                 ylabel='Loss',\n",
    "                                 title='Training Loss',\n",
    "                                 legend=['Loss']))\n",
    "\n",
    "# Setup Model\n",
    "model = get_model(args.arch, n_classes)\n",
    "\n",
    "# model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "# model.cuda()\n",
    "\n",
    "# Check if model has custom optimizer / loss\n",
    "if hasattr(model.module, 'optimizer'):\n",
    "    optimizer = model.module.optimizer\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args.l_rate, momentum=0.99, weight_decay=5e-4)\n",
    "\n",
    "if hasattr(model.module, 'loss'):\n",
    "    print('Using custom loss')\n",
    "    loss_fn = model.module.loss\n",
    "else:\n",
    "    loss_fn = cross_entropy2d\n",
    "\n",
    "if args.resume is not None:                                         \n",
    "    if os.path.isfile(args.resume):\n",
    "        print(\"Loading model and optimizer from checkpoint '{}'\".format(args.resume))\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        model.load_state_dict(checkpoint['model_state'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "        print(\"Loaded checkpoint '{}' (epoch {})\"                    \n",
    "              .format(args.resume, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"No checkpoint found at '{}'\".format(args.resume)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_iou = -100.0 \n",
    "\n",
    "for i, (color_imgs, depth_imgs, semantic_imgs) in enumerate(trainloader):\n",
    "    images = Variable(color_imgs.cuda())\n",
    "    labels = Variable(semantic_imgs.cuda())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(images)\n",
    "\n",
    "    loss = loss_fn(input=outputs, target=labels)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
