{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import collections\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import scipy.misc as m\n",
    "import scipy.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from NYUDv2Loader import *\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "data_path = '/home/dongwonshin/Desktop/Datasets/NYUDv2/'\n",
    "arg_string = '--arch mynetwork20180516 --batch_size 3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argument setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import torch\n",
    "import visdom\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ptsemseg.models import get_model\n",
    "from ptsemseg.loader import get_loader, get_data_path\n",
    "from ptsemseg.metrics import runningScore\n",
    "from ptsemseg.loss import *\n",
    "from ptsemseg.augmentations import *\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Hyperparams')\n",
    "parser.add_argument('--arch', nargs='?', type=str, default='fcn8s', help='Architecture to use [\\'fcn8s, unet, segnet etc\\']')\n",
    "parser.add_argument('--img_rows', nargs='?', type=int, default=256, help='Height of the input image')\n",
    "parser.add_argument('--img_cols', nargs='?', type=int, default=256, help='Width of the input image')\n",
    "\n",
    "parser.add_argument('--img_norm', dest='img_norm', action='store_true', help='Enable input image scales normalization [0, 1] | True by default')\n",
    "parser.add_argument('--no-img_norm', dest='img_norm', action='store_false', help='Disable input image scales normalization [0, 1] | True by default')\n",
    "parser.set_defaults(img_norm=True)\n",
    "\n",
    "parser.add_argument('--n_epoch', nargs='?', type=int, default=100, help='# of the epochs')\n",
    "parser.add_argument('--batch_size', nargs='?', type=int, default=1, help='Batch Size')\n",
    "parser.add_argument('--l_rate', nargs='?', type=float, default=1e-5, help='Learning Rate')\n",
    "parser.add_argument('--feature_scale', nargs='?', type=int, default=1, help='Divider for # of features to use')\n",
    "parser.add_argument('--resume', nargs='?', type=str, default=None, help='Path to previous saved model to restart from')\n",
    "\n",
    "parser.add_argument('--visdom', dest='visdom', action='store_true', help='Enable visualization(s) on visdom | False by default')\n",
    "parser.add_argument('--no-visdom', dest='visdom', action='store_false', help='Disable visualization(s) on visdom | False by default')\n",
    "parser.set_defaults(visdom=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(arg_string.split(' '))\n",
    "\n",
    "# Setup Augmentations\n",
    "data_aug= Compose([RandomRotate(10), RandomHorizontallyFlip()])\n",
    "\n",
    "# Setup Dataloader\n",
    "t_loader = NYUDv2Loader(data_path, is_transform=True)\n",
    "v_loader = NYUDv2Loader(data_path, is_transform=True, split='val')\n",
    "\n",
    "n_classes = t_loader.n_classes\n",
    "trainloader = data.DataLoader(t_loader, batch_size=args.batch_size, num_workers=16, shuffle=True)\n",
    "valloader = data.DataLoader(v_loader, batch_size=args.batch_size, num_workers=16)\n",
    "\n",
    "# Setup Metrics\n",
    "running_metrics = runningScore(n_classes)\n",
    "\n",
    "# Setup visdom for visualization\n",
    "if args.visdom:\n",
    "    vis = visdom.Visdom()\n",
    "\n",
    "    loss_window = vis.line(X=torch.zeros((1,)).cpu(),\n",
    "                       Y=torch.zeros((1)).cpu(),\n",
    "                       opts=dict(xlabel='minibatches',\n",
    "                                 ylabel='Loss',\n",
    "                                 title='Training Loss',\n",
    "                                 legend=['Loss']))\n",
    "\n",
    "# Setup Model\n",
    "model = get_model(args.arch, n_classes)\n",
    "\n",
    "model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "model.cuda()\n",
    "\n",
    "# Check if model has custom optimizer / loss\n",
    "if hasattr(model.module, 'optimizer'):\n",
    "    optimizer = model.module.optimizer\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args.l_rate, momentum=0.99, weight_decay=5e-4)\n",
    "\n",
    "if hasattr(model.module, 'loss'):\n",
    "    print('Using custom loss')\n",
    "    loss_fn = model.module.loss\n",
    "else:\n",
    "    loss_fn = cross_entropy2d\n",
    "\n",
    "if args.resume is not None:                                         \n",
    "    if os.path.isfile(args.resume):\n",
    "        print(\"Loading model and optimizer from checkpoint '{}'\".format(args.resume))\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        model.load_state_dict(checkpoint['model_state'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "        print(\"Loaded checkpoint '{}' (epoch {})\"                    \n",
    "              .format(args.resume, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"No checkpoint found at '{}'\".format(args.resume)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_iou = -100.0 \n",
    "for epoch in range(args.n_epoch):\n",
    "    model.train()\n",
    "    for i, (color_imgs, depth_imgs, label_imgs) in enumerate(trainloader):\n",
    "        images = Variable(color_imgs.cuda())\n",
    "        depth_images = Variable(depth_imgs.cuda())\n",
    "        labels = Variable(label_imgs.cuda())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images,depth_images)\n",
    "\n",
    "        loss = loss_fn(input=outputs, target=labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if args.visdom:\n",
    "            vis.line(\n",
    "                X=torch.ones((1, 1)).cpu() * i,\n",
    "                Y=torch.Tensor([loss.data[0]]).unsqueeze(0).cpu(),\n",
    "                win=loss_window,\n",
    "                update='append')\n",
    "\n",
    "        if (i+1) % 20 == 0:\n",
    "            print(\"Epoch [%d/%d] Loss: %.4f\" % (epoch+1, args.n_epoch, loss.data[0]))\n",
    "\n",
    "    model.eval()\n",
    "    for i_val, (color_images_val, depth_images_val, label_images_val) in tqdm(enumerate(valloader)):\n",
    "        color_images_val = Variable(color_images_val.cuda(), volatile=True)\n",
    "        depth_images_val = Variable(depth_images_val.cuda(), volatile=True)\n",
    "        label_images_val = Variable(label_images_val.cuda(), volatile=True)\n",
    "\n",
    "        outputs = model(color_images_val, depth_images_val)\n",
    "        pred = outputs.data.max(1)[1].cpu().numpy()\n",
    "        gt = label_images_val.data.cpu().numpy()\n",
    "        running_metrics.update(gt, pred)\n",
    "\n",
    "    score, class_iou = running_metrics.get_scores()\n",
    "    for k, v in score.items():\n",
    "        print(k, v)\n",
    "    running_metrics.reset()\n",
    "\n",
    "    if score['Mean IoU : \\t'] >= best_iou:\n",
    "        best_iou = score['Mean IoU : \\t']\n",
    "        state = {'epoch': epoch+1,\n",
    "                 'model_state': model.state_dict(),\n",
    "                 'optimizer_state' : optimizer.state_dict(),}\n",
    "        torch.save(state, \"../model_weights/{}_{}_best_model.pkl\".format(args.arch, 'NYUDv2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training.. fcn8s_rgbd_maskedconv on NYUDv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
