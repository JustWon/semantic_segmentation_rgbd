{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy.misc as m\n",
    "from torch.utils import data\n",
    "\n",
    "def recursive_glob(rootdir='.', suffix=''):\n",
    "    \"\"\"Performs recursive glob with given suffix and rootdir \n",
    "        :param rootdir is the root directory\n",
    "        :param suffix is the suffix to be searched\n",
    "    \"\"\"\n",
    "    return [os.path.join(looproot, filename)\n",
    "        for looproot, _, filenames in os.walk(rootdir)\n",
    "        for filename in filenames if filename.endswith(suffix)]\n",
    "\n",
    "class NYUDv2Loader(data.Dataset):\n",
    "    def __init__(self, root, split=\"training\", is_transform=False, img_size=(480, 640), img_norm=True):\n",
    "        self.root = root\n",
    "        self.is_transform = is_transform\n",
    "        self.n_classes = 13\n",
    "        self.img_norm = img_norm\n",
    "        self.img_size = img_size if isinstance(img_size, tuple) else (img_size, img_size)\n",
    "        self.mean = np.array([104.00699, 116.66877, 122.67892])\n",
    "        self.depth_mean = 0\n",
    "        self.color_files = collections.defaultdict(list)\n",
    "        self.depth_files = collections.defaultdict(list)\n",
    "        self.label_files = collections.defaultdict(list)\n",
    "        self.cmap = self.color_map(normalized=False)\n",
    "\n",
    "        split_map = {\"training\": 'train', \"val\": 'test',}\n",
    "        self.split = split_map[split]\n",
    "\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            file_list =  sorted(recursive_glob(rootdir=self.root + split +'/color/', suffix='png'))\n",
    "            self.color_files[split] = file_list\n",
    "        \n",
    "        for split in [\"train\", \"test\"]:\n",
    "            file_list =  sorted(recursive_glob(rootdir=self.root + split +'/depth/', suffix='npy'))\n",
    "            self.depth_files[split] = file_list    \n",
    "        \n",
    "        for split in [\"train\", \"test\"]:\n",
    "            file_list =  sorted(recursive_glob(rootdir=self.root + split +'/label/', suffix='png'))\n",
    "            self.label_files[split] = file_list\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.color_files[self.split])\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        color_path = self.color_files[self.split][index].rstrip()\n",
    "        depth_path = self.depth_files[self.split][index].rstrip()\n",
    "        label_path = self.label_files[self.split][index].rstrip()\n",
    "\n",
    "        color_img = m.imread(color_path)    \n",
    "        color_img = np.array(color_img, dtype=np.uint8)\n",
    "\n",
    "        depth_img = np.load(depth_path)    \n",
    "        depth_img = np.array(depth_img)\n",
    "        \n",
    "        label_img = m.imread(label_path)    \n",
    "        label_img = np.array(label_img, dtype=np.uint8)\n",
    "        \n",
    "        if self.is_transform:\n",
    "            color_img, depth_img, label_img = self.transform(color_img, depth_img, label_img)\n",
    "        \n",
    "        return color_img, depth_img, label_img\n",
    "\n",
    "\n",
    "    def transform(self, img, depth_img, lbl):\n",
    "        img = m.imresize(img, (self.img_size[0], self.img_size[1])) # uint8 with RGB mode\n",
    "        img = img[:, :, ::-1] # RGB -> BGR\n",
    "        img = img.astype(np.float64)\n",
    "        img -= self.mean\n",
    "        if self.img_norm:\n",
    "            # Resize scales images from 0 to 255, thus we need\n",
    "            # to divide by 255.0\n",
    "            img = img.astype(float) / 255.0\n",
    "        # NHWC -> NCHW\n",
    "        img = img.transpose(2, 0, 1)\n",
    "        \n",
    "        depth_img = m.imresize(depth_img, (self.img_size[0], self.img_size[1]))\n",
    "        depth_img = depth_img.astype(np.float64)\n",
    "        depth_img = depth_img[np.newaxis,:]\n",
    "        depth_img -= self.depth_mean\n",
    "        if self.img_norm:\n",
    "            # Resize scales images from 0 to 255, thus we need\n",
    "            # to divide by 255.0\n",
    "            depth_img = depth_img.astype(float) / 255.0\n",
    "        \n",
    "\n",
    "        classes = np.unique(lbl)\n",
    "        lbl = lbl.astype(float)\n",
    "        lbl = m.imresize(lbl, (self.img_size[0], self.img_size[1]), 'nearest', mode='F')\n",
    "        lbl = lbl[np.newaxis,:]\n",
    "        lbl = lbl.astype(int)\n",
    "        assert(np.all(classes == np.unique(lbl)))\n",
    "\n",
    "        img = torch.from_numpy(img).float()\n",
    "        depth_img = torch.from_numpy(depth_img).float()\n",
    "        lbl = torch.from_numpy(lbl).long()\n",
    "        return img, depth_img, lbl\n",
    "\n",
    "\n",
    "    def color_map(self, N=256, normalized=False):\n",
    "        \"\"\"\n",
    "        Return Color Map in PASCAL VOC format\n",
    "        \"\"\"\n",
    "\n",
    "        def bitget(byteval, idx):\n",
    "            return ((byteval & (1 << idx)) != 0)\n",
    "\n",
    "        dtype = 'float32' if normalized else 'uint8'\n",
    "        cmap = np.zeros((N, 3), dtype=dtype)\n",
    "        for i in range(N):\n",
    "            r = g = b = 0\n",
    "            c = i\n",
    "            for j in range(8):\n",
    "                r = r | (bitget(c, 0) << 7-j)\n",
    "                g = g | (bitget(c, 1) << 7-j)\n",
    "                b = b | (bitget(c, 2) << 7-j)\n",
    "                c = c >> 3\n",
    "\n",
    "            cmap[i] = np.array([r, g, b])\n",
    "\n",
    "        cmap = cmap/255.0 if normalized else cmap\n",
    "        return cmap\n",
    "\n",
    "\n",
    "    def decode_segmap(self, temp):\n",
    "        r = temp.copy()\n",
    "        g = temp.copy()\n",
    "        b = temp.copy()\n",
    "        for l in range(0, self.n_classes):\n",
    "            r[temp == l] = self.cmap[l,0]\n",
    "            g[temp == l] = self.cmap[l,1]\n",
    "            b[temp == l] = self.cmap[l,2]\n",
    "\n",
    "        rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n",
    "        rgb[:, :, 0] = r / 255.0\n",
    "        rgb[:, :, 1] = g / 255.0\n",
    "        rgb[:, :, 2] = b / 255.0\n",
    "        return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/dongwonshin/Desktop/Datasets/NYUDv2/'\n",
    "t_loader = NYUDv2Loader(data_path, is_transform=True)\n",
    "v_loader = NYUDv2Loader(data_path, is_transform=True, split='val')\n",
    "\n",
    "n_classes = t_loader.n_classes\n",
    "trainloader = data.DataLoader(t_loader, batch_size=3, num_workers=16, shuffle=True)\n",
    "valloader = data.DataLoader(v_loader, batch_size=3, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795\n",
      "795\n",
      "795\n"
     ]
    }
   ],
   "source": [
    "print(len(t_loader.color_files['train']))\n",
    "print(len(t_loader.depth_files['train']))\n",
    "print(len(t_loader.label_files['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_imgs, depth_imgs, label_imgs = iter(trainloader).next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYUDv2Loader from python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NYUDv2Loader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/dongwonshin/Desktop/Datasets/NYUDv2/'\n",
    "t_loader = NYUDv2Loader(data_path, is_transform=True)\n",
    "v_loader = NYUDv2Loader(data_path, is_transform=True, split='val')\n",
    "\n",
    "n_classes = t_loader.n_classes\n",
    "trainloader = data.DataLoader(t_loader, batch_size=3, num_workers=16, shuffle=True)\n",
    "valloader = data.DataLoader(v_loader, batch_size=3, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795\n",
      "795\n",
      "795\n"
     ]
    }
   ],
   "source": [
    "print(len(t_loader.color_files['train']))\n",
    "print(len(t_loader.depth_files['train']))\n",
    "print(len(t_loader.label_files['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
