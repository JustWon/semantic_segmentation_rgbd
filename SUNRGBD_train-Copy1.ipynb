{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import collections\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import scipy.misc as m\n",
    "import scipy.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "def recursive_glob(rootdir='.', suffix=''):\n",
    "    \"\"\"Performs recursive glob with given suffix and rootdir \n",
    "        :param rootdir is the root directory\n",
    "        :param suffix is the suffix to be searched\n",
    "    \"\"\"\n",
    "    return [os.path.join(looproot, filename)\n",
    "        for looproot, _, filenames in os.walk(rootdir)\n",
    "        for filename in filenames if filename.endswith(suffix)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SUNRGBDLoader(data.Dataset):\n",
    "    def __init__(self, root, split=\"training\", is_transform=False, img_size=(480, 640), img_norm=True):\n",
    "        self.root = root\n",
    "        self.is_transform = is_transform\n",
    "        self.n_classes = 38\n",
    "        self.img_norm = img_norm\n",
    "        self.img_size = img_size if isinstance(img_size, tuple) else (img_size, img_size)\n",
    "        self.mean = np.array([104.00699, 116.66877, 122.67892])\n",
    "        self.color_files = collections.defaultdict(list)\n",
    "        self.depth_files = collections.defaultdict(list)\n",
    "        self.semantic_files = collections.defaultdict(list)\n",
    "        self.cmap = self.color_map(normalized=False)\n",
    "\n",
    "        split_map = {\"training\": 'train', \"val\": 'test',}\n",
    "        self.split = split_map[split]\n",
    "\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            file_list =  sorted(recursive_glob(rootdir=self.root + '/color/', suffix='jpg'))\n",
    "            self.color_files[split] = file_list\n",
    "        \n",
    "        for split in [\"train\", \"test\"]:\n",
    "            file_list =  sorted(recursive_glob(rootdir=self.root + '/depth/', suffix='png'))\n",
    "            self.depth_files[split] = file_list    \n",
    "        \n",
    "        for split in [\"train\", \"test\"]:\n",
    "            file_list =  sorted(recursive_glob(rootdir=self.root + '/seg/', suffix='png'))\n",
    "            self.semantic_files[split] = file_list\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.color_files[self.split])\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        color_path = self.color_files[self.split][index].rstrip()\n",
    "        depth_path = self.depth_files[self.split][index].rstrip()\n",
    "        semantic_path = self.semantic_files[self.split][index].rstrip()\n",
    "\n",
    "        color_img = m.imread(color_path)    \n",
    "        color_img = np.array(color_img, dtype=np.uint8)\n",
    "\n",
    "        depth_img = m.imread(depth_path)    \n",
    "        depth_img = np.array(depth_img, dtype=np.uint8)\n",
    "        \n",
    "        semantic_img = m.imread(semantic_path)\n",
    "        semantic_img = np.array(semantic_img, dtype=np.uint8)\n",
    "        \n",
    "        if self.is_transform:\n",
    "            color_img, semantic_img = self.transform(color_img, semantic_img)\n",
    "        \n",
    "        return color_img, depth_img, semantic_img\n",
    "\n",
    "\n",
    "    def transform(self, img, lbl):\n",
    "        img = m.imresize(img, (self.img_size[0], self.img_size[1])) # uint8 with RGB mode\n",
    "        img = img[:, :, ::-1] # RGB -> BGR\n",
    "        img = img.astype(np.float64)\n",
    "        img -= self.mean\n",
    "        if self.img_norm:\n",
    "            # Resize scales images from 0 to 255, thus we need\n",
    "            # to divide by 255.0\n",
    "            img = img.astype(float) / 255.0\n",
    "        # NHWC -> NCHW\n",
    "        img = img.transpose(2, 0, 1)\n",
    "\n",
    "        classes = np.unique(lbl)\n",
    "        lbl = lbl.astype(float)\n",
    "        lbl = m.imresize(lbl, (self.img_size[0], self.img_size[1]), 'nearest', mode='F')\n",
    "        lbl = lbl.astype(int)\n",
    "        assert(np.all(classes == np.unique(lbl)))\n",
    "\n",
    "        img = torch.from_numpy(img).float()\n",
    "        lbl = torch.from_numpy(lbl).long()\n",
    "        return img, lbl\n",
    "\n",
    "\n",
    "    def color_map(self, N=256, normalized=False):\n",
    "        \"\"\"\n",
    "        Return Color Map in PASCAL VOC format\n",
    "        \"\"\"\n",
    "\n",
    "        def bitget(byteval, idx):\n",
    "            return ((byteval & (1 << idx)) != 0)\n",
    "\n",
    "        dtype = 'float32' if normalized else 'uint8'\n",
    "        cmap = np.zeros((N, 3), dtype=dtype)\n",
    "        for i in range(N):\n",
    "            r = g = b = 0\n",
    "            c = i\n",
    "            for j in range(8):\n",
    "                r = r | (bitget(c, 0) << 7-j)\n",
    "                g = g | (bitget(c, 1) << 7-j)\n",
    "                b = b | (bitget(c, 2) << 7-j)\n",
    "                c = c >> 3\n",
    "\n",
    "            cmap[i] = np.array([r, g, b])\n",
    "\n",
    "        cmap = cmap/255.0 if normalized else cmap\n",
    "        return cmap\n",
    "\n",
    "\n",
    "    def decode_segmap(self, temp):\n",
    "        r = temp.copy()\n",
    "        g = temp.copy()\n",
    "        b = temp.copy()\n",
    "        for l in range(0, self.n_classes):\n",
    "            r[temp == l] = self.cmap[l,0]\n",
    "            g[temp == l] = self.cmap[l,1]\n",
    "            b[temp == l] = self.cmap[l,2]\n",
    "\n",
    "        rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n",
    "        rgb[:, :, 0] = r / 255.0\n",
    "        rgb[:, :, 1] = g / 255.0\n",
    "        rgb[:, :, 2] = b / 255.0\n",
    "        return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import torch\n",
    "import visdom\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ptsemseg.models import get_model\n",
    "from ptsemseg.loader import get_loader, get_data_path\n",
    "from ptsemseg.metrics import runningScore\n",
    "from ptsemseg.loss import *\n",
    "from ptsemseg.augmentations import *\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Hyperparams')\n",
    "parser.add_argument('--arch', nargs='?', type=str, default='fcn8s', \n",
    "                    help='Architecture to use [\\'fcn8s, unet, segnet etc\\']')\n",
    "parser.add_argument('--img_rows', nargs='?', type=int, default=256, \n",
    "                    help='Height of the input image')\n",
    "parser.add_argument('--img_cols', nargs='?', type=int, default=256, \n",
    "                    help='Width of the input image')\n",
    "\n",
    "parser.add_argument('--img_norm', dest='img_norm', action='store_true', \n",
    "                    help='Enable input image scales normalization [0, 1] | True by default')\n",
    "parser.add_argument('--no-img_norm', dest='img_norm', action='store_false', \n",
    "                    help='Disable input image scales normalization [0, 1] | True by default')\n",
    "parser.set_defaults(img_norm=True)\n",
    "\n",
    "parser.add_argument('--n_epoch', nargs='?', type=int, default=100, \n",
    "                    help='# of the epochs')\n",
    "parser.add_argument('--batch_size', nargs='?', type=int, default=1, \n",
    "                    help='Batch Size')\n",
    "parser.add_argument('--l_rate', nargs='?', type=float, default=1e-5, \n",
    "                    help='Learning Rate')\n",
    "parser.add_argument('--feature_scale', nargs='?', type=int, default=1, \n",
    "                    help='Divider for # of features to use')\n",
    "parser.add_argument('--resume', nargs='?', type=str, default=None,    \n",
    "                    help='Path to previous saved model to restart from')\n",
    "\n",
    "parser.add_argument('--visdom', dest='visdom', action='store_true', \n",
    "                    help='Enable visualization(s) on visdom | False by default')\n",
    "parser.add_argument('--no-visdom', dest='visdom', action='store_false', \n",
    "                    help='Disable visualization(s) on visdom | False by default')\n",
    "parser.set_defaults(visdom=False)\n",
    "\n",
    "# args = parser.parse_args('--arch fcn8s'.split(' '))\n",
    "# train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args('--arch fcn8s'.split(' '))\n",
    "\n",
    "# Setup Augmentations\n",
    "data_aug= Compose([RandomRotate(10),                                        \n",
    "                   RandomHorizontallyFlip()])\n",
    "\n",
    "# Setup Dataloader\n",
    "data_path = '/home/dongwonshin/Desktop/Datasets/SUN_RGBD(organized)/temp'\n",
    "t_loader = SUNRGBDLoader(data_path, is_transform=True)\n",
    "v_loader = SUNRGBDLoader(data_path, is_transform=True, split='val')\n",
    "\n",
    "n_classes = t_loader.n_classes\n",
    "trainloader = data.DataLoader(t_loader, batch_size=args.batch_size, num_workers=16, shuffle=True)\n",
    "valloader = data.DataLoader(v_loader, batch_size=args.batch_size, num_workers=16)\n",
    "\n",
    "# Setup Metrics\n",
    "running_metrics = runningScore(n_classes)\n",
    "\n",
    "# Setup visdom for visualization\n",
    "if args.visdom:\n",
    "    vis = visdom.Visdom()\n",
    "\n",
    "    loss_window = vis.line(X=torch.zeros((1,)).cpu(),\n",
    "                       Y=torch.zeros((1)).cpu(),\n",
    "                       opts=dict(xlabel='minibatches',\n",
    "                                 ylabel='Loss',\n",
    "                                 title='Training Loss',\n",
    "                                 legend=['Loss']))\n",
    "\n",
    "# Setup Model\n",
    "model = get_model(args.arch, n_classes)\n",
    "\n",
    "model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "model.cuda()\n",
    "\n",
    "# Check if model has custom optimizer / loss\n",
    "if hasattr(model.module, 'optimizer'):\n",
    "    optimizer = model.module.optimizer\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args.l_rate, momentum=0.99, weight_decay=5e-4)\n",
    "\n",
    "if hasattr(model.module, 'loss'):\n",
    "    print('Using custom loss')\n",
    "    loss_fn = model.module.loss\n",
    "else:\n",
    "    loss_fn = cross_entropy2d\n",
    "\n",
    "if args.resume is not None:                                         \n",
    "    if os.path.isfile(args.resume):\n",
    "        print(\"Loading model and optimizer from checkpoint '{}'\".format(args.resume))\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        model.load_state_dict(checkpoint['model_state'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "        print(\"Loaded checkpoint '{}' (epoch {})\"                    \n",
    "              .format(args.resume, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"No checkpoint found at '{}'\".format(args.resume)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_imgs, depth_imgs, semantic_imgs = iter(trainloader).next()\n",
    "images = Variable(color_imgs.cuda())\n",
    "labels = Variable(semantic_imgs.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongwonshin/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1462: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.upsample instead.\n",
      "  warnings.warn(\"nn.functional.upsample_bilinear is deprecated. Use nn.functional.upsample instead.\")\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "outputs = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = outputs\n",
    "target = labels\n",
    "n, c, h, w = input.size()\n",
    "nt, ht, wt = target.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle inconsistent size between input and target\n",
    "if h > ht and w > wt: # upsample labels\n",
    "    target = target.unsequeeze(1)\n",
    "    target = F.upsample(target, size=(h, w), mode='nearest')\n",
    "    target = target.sequeeze(1)\n",
    "elif h < ht and w < wt: # upsample images\n",
    "    input = F.upsample(input, size=(ht, wt), mode='bilinear')\n",
    "elif h != ht and w != wt:\n",
    "    raise Exception(\"Only support upsampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_p = F.log_softmax(input, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_p = log_p.transpose(1, 2).transpose(2, 3).contiguous().view(-1, c)\n",
    "log_p = log_p[target.view(-1, 1).repeat(1, c) >= 0]\n",
    "log_p = log_p.view(-1, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = target >= 0\n",
    "# target = target[mask]\n",
    "target = Variable(torch.cuda.LongTensor(307200).fill_(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2018년 05월 08일 11시 17분 56초에 추가\n",
    "확실히.. 세만틱 이미지를 불러와서 쓰는데 문제가 있다..\n",
    "클래스 값이.. 저장된것이 색상으로 저장되어서.. \n",
    "그게 로스 함수 계산할때.. 문제가된다.. \n",
    "이것을 컨버팅 해주는 루틴이 필요하다..\n",
    "\n",
    "SUNRGBDloader의 transform에서 작업을 해줘야할것같다.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " -5.0421  -4.8467  -8.9306  ...   -6.6899  -3.6626  -6.9615\n",
       " -4.9404  -4.5488  -8.5733  ...   -6.4881  -3.5655  -6.7985\n",
       " -4.8502  -4.2623  -8.2276  ...   -6.2978  -3.4798  -6.6470\n",
       "           ...               ⋱              ...            \n",
       " -5.3898  -4.3915  -4.2632  ...   -5.0559  -3.6877  -6.3092\n",
       " -5.4211  -4.4630  -4.3092  ...   -5.1669  -3.7845  -6.4959\n",
       " -5.4552  -4.5374  -4.3581  ...   -5.2808  -3.8841  -6.6855\n",
       "[torch.cuda.FloatTensor of size 307200x38 (GPU 0)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0\n",
       " 0\n",
       " 0\n",
       "⋮ \n",
       " 0\n",
       " 0\n",
       " 0\n",
       "[torch.cuda.LongTensor of size 307200 (GPU 0)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.nll_loss(log_p, target, ignore_index=250, size_average=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 4.8452\n",
       "[torch.cuda.FloatTensor of size 1 (GPU 0)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss /= mask.data.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = loss_fn(input=outputs, target=labels)\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongwonshin/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1462: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.upsample instead.\n",
      "  warnings.warn(\"nn.functional.upsample_bilinear is deprecated. Use nn.functional.upsample instead.\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1522182087074/work/torch/lib/THC/generated/../THCReduceAll.cuh:339",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-235acbc81011>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pytorch_dataloader/ptsemseg/loss.py\u001b[0m in \u001b[0;36mcross_entropy2d\u001b[0;34m(input, target, weight, size_average)\u001b[0m\n\u001b[1;32m     29\u001b[0m                       weight=weight, size_average=False)\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1522182087074/work/torch/lib/THC/generated/../THCReduceAll.cuh:339"
     ]
    }
   ],
   "source": [
    "best_iou = -100.0 \n",
    "model.train()\n",
    "for i, (color_imgs, depth_imgs, semantic_imgs) in enumerate(trainloader):\n",
    "    images = Variable(color_imgs.cuda())\n",
    "    labels = Variable(semantic_imgs.cuda())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(images)\n",
    "\n",
    "    loss = loss_fn(input=outputs, target=labels)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_img = m.imread('/home/dongwonshin/Desktop/Datasets/SUN_RGBD(organized)/temp/seg/NYUdata_NYU1408_seg.png')\n",
    "semantic_img = np.array(semantic_img, dtype=np.uint8)\n",
    "\n",
    "lbl = semantic_img\n",
    "\n",
    "classes = np.unique(lbl)\n",
    "lbl = lbl.astype(float)\n",
    "lbl = m.imresize(lbl, (480, 640), 'nearest', mode='F')\n",
    "lbl = lbl.astype(int)\n",
    "assert(np.all(classes == np.unique(lbl)))\n",
    "\n",
    "lbl = torch.from_numpy(lbl).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   2,   4,   8,  12,  15,  27,  42,  49,  65, 109, 255])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUNRGBD 데이터셋에서 segmentation 이미지를 mat파일 형태로 제공..\n",
    "근데 예전에 바꿀때.. 저장을 클래스 레이블로 하지않고.. 자동으로 색상으로 변경되어서 저장된것같다..\n",
    "다시 organization을 해줘야할것같다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "mat_file= scipy.io.loadmat('/home/dongwonshin/Desktop/Datasets/SUNRGBD2/SUNRGBD/kv1/b3dodata/img_0063/seg.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__globals__': [],\n",
       " '__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Mon May  4 15:31:06 2015',\n",
       " '__version__': '1.0',\n",
       " 'names': array([[array(['wall'], \n",
       "       dtype='<U4'),\n",
       "         array(['shelf'], \n",
       "       dtype='<U5'),\n",
       "         array(['shelf'], \n",
       "       dtype='<U5'),\n",
       "         array(['floor'], \n",
       "       dtype='<U5'),\n",
       "         array(['chair'], \n",
       "       dtype='<U5'),\n",
       "         array(['books'], \n",
       "       dtype='<U5'),\n",
       "         array(['books'], \n",
       "       dtype='<U5'),\n",
       "         array(['box'], \n",
       "       dtype='<U3'),\n",
       "         array(['books'], \n",
       "       dtype='<U5')]], dtype=object),\n",
       " 'seglabel': array([[2, 2, 2, ..., 1, 1, 1],\n",
       "        [2, 2, 2, ..., 1, 1, 1],\n",
       "        [2, 2, 2, ..., 1, 1, 1],\n",
       "        ..., \n",
       "        [2, 2, 2, ..., 0, 0, 0],\n",
       "        [2, 2, 2, ..., 0, 0, 0],\n",
       "        [2, 2, 2, ..., 0, 0, 0]], dtype=uint8)}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe0d8746cc0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe0db36f4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mat_file['seglabel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
