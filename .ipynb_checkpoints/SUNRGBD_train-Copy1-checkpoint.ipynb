{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import collections\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import scipy.misc as m\n",
    "import scipy.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "def recursive_glob(rootdir='.', suffix=''):\n",
    "    \"\"\"Performs recursive glob with given suffix and rootdir \n",
    "        :param rootdir is the root directory\n",
    "        :param suffix is the suffix to be searched\n",
    "    \"\"\"\n",
    "    return [os.path.join(looproot, filename)\n",
    "        for looproot, _, filenames in os.walk(rootdir)\n",
    "        for filename in filenames if filename.endswith(suffix)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SUNRGBDLoader(data.Dataset):\n",
    "    def __init__(self, root, split=\"training\", is_transform=False, img_size=(480, 640), img_norm=True):\n",
    "        self.root = root\n",
    "        self.is_transform = is_transform\n",
    "        self.n_classes = 38\n",
    "        self.img_norm = img_norm\n",
    "        self.img_size = img_size if isinstance(img_size, tuple) else (img_size, img_size)\n",
    "        self.mean = np.array([104.00699, 116.66877, 122.67892])\n",
    "        self.color_files = collections.defaultdict(list)\n",
    "        self.depth_files = collections.defaultdict(list)\n",
    "        self.semantic_files = collections.defaultdict(list)\n",
    "        self.cmap = self.color_map(normalized=False)\n",
    "\n",
    "        split_map = {\"training\": 'train', \"val\": 'test',}\n",
    "        self.split = split_map[split]\n",
    "\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            file_list =  sorted(recursive_glob(rootdir=self.root + '/color/', suffix='jpg'))\n",
    "            self.color_files[split] = file_list\n",
    "        \n",
    "        for split in [\"train\", \"test\"]:\n",
    "            file_list =  sorted(recursive_glob(rootdir=self.root + '/depth/', suffix='png'))\n",
    "            self.depth_files[split] = file_list    \n",
    "        \n",
    "        for split in [\"train\", \"test\"]:\n",
    "            file_list =  sorted(recursive_glob(rootdir=self.root + '/seg/', suffix='png'))\n",
    "            self.semantic_files[split] = file_list\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.color_files[self.split])\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        color_path = self.color_files[self.split][index].rstrip()\n",
    "        depth_path = self.depth_files[self.split][index].rstrip()\n",
    "        semantic_path = self.semantic_files[self.split][index].rstrip()\n",
    "\n",
    "        color_img = m.imread(color_path)    \n",
    "        color_img = np.array(color_img, dtype=np.uint8)\n",
    "\n",
    "        depth_img = m.imread(depth_path)    \n",
    "        depth_img = np.array(depth_img, dtype=np.uint8)\n",
    "        \n",
    "        semantic_img = m.imread(semantic_path)\n",
    "        semantic_img = np.array(semantic_img, dtype=np.uint8)\n",
    "        \n",
    "        if self.is_transform:\n",
    "            color_img, semantic_img = self.transform(color_img, semantic_img)\n",
    "        \n",
    "        return color_img, depth_img, semantic_img\n",
    "\n",
    "\n",
    "    def transform(self, img, lbl):\n",
    "        img = m.imresize(img, (self.img_size[0], self.img_size[1])) # uint8 with RGB mode\n",
    "        img = img[:, :, ::-1] # RGB -> BGR\n",
    "        img = img.astype(np.float64)\n",
    "        img -= self.mean\n",
    "        if self.img_norm:\n",
    "            # Resize scales images from 0 to 255, thus we need\n",
    "            # to divide by 255.0\n",
    "            img = img.astype(float) / 255.0\n",
    "        # NHWC -> NCHW\n",
    "        img = img.transpose(2, 0, 1)\n",
    "\n",
    "        classes = np.unique(lbl)\n",
    "        lbl = lbl.astype(float)\n",
    "        lbl = m.imresize(lbl, (self.img_size[0], self.img_size[1]), 'nearest', mode='F')\n",
    "        lbl = lbl.astype(int)\n",
    "        assert(np.all(classes == np.unique(lbl)))\n",
    "\n",
    "        img = torch.from_numpy(img).float()\n",
    "        lbl = torch.from_numpy(lbl).long()\n",
    "        return img, lbl\n",
    "\n",
    "\n",
    "    def color_map(self, N=256, normalized=False):\n",
    "        \"\"\"\n",
    "        Return Color Map in PASCAL VOC format\n",
    "        \"\"\"\n",
    "\n",
    "        def bitget(byteval, idx):\n",
    "            return ((byteval & (1 << idx)) != 0)\n",
    "\n",
    "        dtype = 'float32' if normalized else 'uint8'\n",
    "        cmap = np.zeros((N, 3), dtype=dtype)\n",
    "        for i in range(N):\n",
    "            r = g = b = 0\n",
    "            c = i\n",
    "            for j in range(8):\n",
    "                r = r | (bitget(c, 0) << 7-j)\n",
    "                g = g | (bitget(c, 1) << 7-j)\n",
    "                b = b | (bitget(c, 2) << 7-j)\n",
    "                c = c >> 3\n",
    "\n",
    "            cmap[i] = np.array([r, g, b])\n",
    "\n",
    "        cmap = cmap/255.0 if normalized else cmap\n",
    "        return cmap\n",
    "\n",
    "\n",
    "    def decode_segmap(self, temp):\n",
    "        r = temp.copy()\n",
    "        g = temp.copy()\n",
    "        b = temp.copy()\n",
    "        for l in range(0, self.n_classes):\n",
    "            r[temp == l] = self.cmap[l,0]\n",
    "            g[temp == l] = self.cmap[l,1]\n",
    "            b[temp == l] = self.cmap[l,2]\n",
    "\n",
    "        rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n",
    "        rgb[:, :, 0] = r / 255.0\n",
    "        rgb[:, :, 1] = g / 255.0\n",
    "        rgb[:, :, 2] = b / 255.0\n",
    "        return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import torch\n",
    "import visdom\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ptsemseg.models import get_model\n",
    "from ptsemseg.loader import get_loader, get_data_path\n",
    "from ptsemseg.metrics import runningScore\n",
    "from ptsemseg.loss import *\n",
    "from ptsemseg.augmentations import *\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Hyperparams')\n",
    "parser.add_argument('--arch', nargs='?', type=str, default='fcn8s', \n",
    "                    help='Architecture to use [\\'fcn8s, unet, segnet etc\\']')\n",
    "parser.add_argument('--img_rows', nargs='?', type=int, default=256, \n",
    "                    help='Height of the input image')\n",
    "parser.add_argument('--img_cols', nargs='?', type=int, default=256, \n",
    "                    help='Width of the input image')\n",
    "\n",
    "parser.add_argument('--img_norm', dest='img_norm', action='store_true', \n",
    "                    help='Enable input image scales normalization [0, 1] | True by default')\n",
    "parser.add_argument('--no-img_norm', dest='img_norm', action='store_false', \n",
    "                    help='Disable input image scales normalization [0, 1] | True by default')\n",
    "parser.set_defaults(img_norm=True)\n",
    "\n",
    "parser.add_argument('--n_epoch', nargs='?', type=int, default=100, \n",
    "                    help='# of the epochs')\n",
    "parser.add_argument('--batch_size', nargs='?', type=int, default=1, \n",
    "                    help='Batch Size')\n",
    "parser.add_argument('--l_rate', nargs='?', type=float, default=1e-5, \n",
    "                    help='Learning Rate')\n",
    "parser.add_argument('--feature_scale', nargs='?', type=int, default=1, \n",
    "                    help='Divider for # of features to use')\n",
    "parser.add_argument('--resume', nargs='?', type=str, default=None,    \n",
    "                    help='Path to previous saved model to restart from')\n",
    "\n",
    "parser.add_argument('--visdom', dest='visdom', action='store_true', \n",
    "                    help='Enable visualization(s) on visdom | False by default')\n",
    "parser.add_argument('--no-visdom', dest='visdom', action='store_false', \n",
    "                    help='Disable visualization(s) on visdom | False by default')\n",
    "parser.set_defaults(visdom=False)\n",
    "\n",
    "# args = parser.parse_args('--arch fcn8s'.split(' '))\n",
    "# train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args('--arch fcn8s'.split(' '))\n",
    "\n",
    "# Setup Augmentations\n",
    "data_aug= Compose([RandomRotate(10),                                        \n",
    "                   RandomHorizontallyFlip()])\n",
    "\n",
    "# Setup Dataloader\n",
    "data_path = '/home/dongwonshin/Desktop/Datasets/SUN_RGBD(organized)/temp'\n",
    "t_loader = SUNRGBDLoader(data_path, is_transform=True)\n",
    "v_loader = SUNRGBDLoader(data_path, is_transform=True, split='val')\n",
    "\n",
    "n_classes = t_loader.n_classes\n",
    "trainloader = data.DataLoader(t_loader, batch_size=args.batch_size, num_workers=16, shuffle=True)\n",
    "valloader = data.DataLoader(v_loader, batch_size=args.batch_size, num_workers=16)\n",
    "\n",
    "# Setup Metrics\n",
    "running_metrics = runningScore(n_classes)\n",
    "\n",
    "# Setup visdom for visualization\n",
    "if args.visdom:\n",
    "    vis = visdom.Visdom()\n",
    "\n",
    "    loss_window = vis.line(X=torch.zeros((1,)).cpu(),\n",
    "                       Y=torch.zeros((1)).cpu(),\n",
    "                       opts=dict(xlabel='minibatches',\n",
    "                                 ylabel='Loss',\n",
    "                                 title='Training Loss',\n",
    "                                 legend=['Loss']))\n",
    "\n",
    "# Setup Model\n",
    "model = get_model(args.arch, n_classes)\n",
    "\n",
    "model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "model.cuda()\n",
    "\n",
    "# Check if model has custom optimizer / loss\n",
    "if hasattr(model.module, 'optimizer'):\n",
    "    optimizer = model.module.optimizer\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args.l_rate, momentum=0.99, weight_decay=5e-4)\n",
    "\n",
    "if hasattr(model.module, 'loss'):\n",
    "    print('Using custom loss')\n",
    "    loss_fn = model.module.loss\n",
    "else:\n",
    "    loss_fn = cross_entropy2d\n",
    "\n",
    "if args.resume is not None:                                         \n",
    "    if os.path.isfile(args.resume):\n",
    "        print(\"Loading model and optimizer from checkpoint '{}'\".format(args.resume))\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        model.load_state_dict(checkpoint['model_state'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "        print(\"Loaded checkpoint '{}' (epoch {})\"                    \n",
    "              .format(args.resume, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"No checkpoint found at '{}'\".format(args.resume)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_imgs, depth_imgs, semantic_imgs = iter(trainloader).next()\n",
    "images = Variable(color_imgs.cuda())\n",
    "labels = Variable(semantic_imgs.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongwonshin/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1462: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.upsample instead.\n",
      "  warnings.warn(\"nn.functional.upsample_bilinear is deprecated. Use nn.functional.upsample instead.\")\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "outputs = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = outputs\n",
    "target = labels\n",
    "n, c, h, w = input.size()\n",
    "nt, ht, wt = target.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle inconsistent size between input and target\n",
    "if h > ht and w > wt: # upsample labels\n",
    "    target = target.unsequeeze(1)\n",
    "    target = F.upsample(target, size=(h, w), mode='nearest')\n",
    "    target = target.sequeeze(1)\n",
    "elif h < ht and w < wt: # upsample images\n",
    "    input = F.upsample(input, size=(ht, wt), mode='bilinear')\n",
    "elif h != ht and w != wt:\n",
    "    raise Exception(\"Only support upsampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_p = F.log_softmax(input, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_p = log_p.transpose(1, 2).transpose(2, 3).contiguous().view(-1, c)\n",
    "log_p = log_p[target.view(-1, 1).repeat(1, c) >= 0]\n",
    "log_p = log_p.view(-1, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = target >= 0\n",
    "# target = target[mask]\n",
    "target = Variable(torch.cuda.LongTensor(307200).fill_(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2018년 05월 08일 11시 17분 56초에 추가\n",
    "확실히.. 세만틱 이미지를 불러와서 쓰는데 문제가 있다..\n",
    "클래스 값이.. 저장된것이 색상으로 저장되어서.. \n",
    "그게 로스 함수 계산할때.. 문제가된다.. \n",
    "이것을 컨버팅 해주는 루틴이 필요하다..\n",
    "\n",
    "SUNRGBDloader의 transform에서 작업을 해줘야할것같다.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " -5.0421  -4.8467  -8.9306  ...   -6.6899  -3.6626  -6.9615\n",
       " -4.9404  -4.5488  -8.5733  ...   -6.4881  -3.5655  -6.7985\n",
       " -4.8502  -4.2623  -8.2276  ...   -6.2978  -3.4798  -6.6470\n",
       "           ...               ⋱              ...            \n",
       " -5.3898  -4.3915  -4.2632  ...   -5.0559  -3.6877  -6.3092\n",
       " -5.4211  -4.4630  -4.3092  ...   -5.1669  -3.7845  -6.4959\n",
       " -5.4552  -4.5374  -4.3581  ...   -5.2808  -3.8841  -6.6855\n",
       "[torch.cuda.FloatTensor of size 307200x38 (GPU 0)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0\n",
       " 0\n",
       " 0\n",
       "⋮ \n",
       " 0\n",
       " 0\n",
       " 0\n",
       "[torch.cuda.LongTensor of size 307200 (GPU 0)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.nll_loss(log_p, target, ignore_index=250, size_average=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 4.8452\n",
       "[torch.cuda.FloatTensor of size 1 (GPU 0)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss /= mask.data.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = loss_fn(input=outputs, target=labels)\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongwonshin/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1462: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.upsample instead.\n",
      "  warnings.warn(\"nn.functional.upsample_bilinear is deprecated. Use nn.functional.upsample instead.\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1522182087074/work/torch/lib/THC/generated/../THCReduceAll.cuh:339",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-235acbc81011>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pytorch_dataloader/ptsemseg/loss.py\u001b[0m in \u001b[0;36mcross_entropy2d\u001b[0;34m(input, target, weight, size_average)\u001b[0m\n\u001b[1;32m     29\u001b[0m                       weight=weight, size_average=False)\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1522182087074/work/torch/lib/THC/generated/../THCReduceAll.cuh:339"
     ]
    }
   ],
   "source": [
    "best_iou = -100.0 \n",
    "model.train()\n",
    "for i, (color_imgs, depth_imgs, semantic_imgs) in enumerate(trainloader):\n",
    "    images = Variable(color_imgs.cuda())\n",
    "    labels = Variable(semantic_imgs.cuda())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(images)\n",
    "\n",
    "    loss = loss_fn(input=outputs, target=labels)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_img = m.imread('/home/dongwonshin/Desktop/Datasets/SUN_RGBD(organized)/temp/seg/NYUdata_NYU1408_seg.png')\n",
    "semantic_img = np.array(semantic_img, dtype=np.uint8)\n",
    "\n",
    "lbl = semantic_img\n",
    "\n",
    "classes = np.unique(lbl)\n",
    "lbl = lbl.astype(float)\n",
    "lbl = m.imresize(lbl, (480, 640), 'nearest', mode='F')\n",
    "lbl = lbl.astype(int)\n",
    "assert(np.all(classes == np.unique(lbl)))\n",
    "\n",
    "lbl = torch.from_numpy(lbl).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   2,   4,   8,  12,  15,  27,  42,  49,  65, 109, 255])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUNRGBD 데이터셋에서 segmentation 이미지를 mat파일 형태로 제공..\n",
    "근데 예전에 바꿀때.. 저장을 클래스 레이블로 하지않고.. 자동으로 색상으로 변경되어서 저장된것같다..\n",
    "다시 organization을 해줘야할것같다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
